import streamlit as st
import tensorflow as tf
import numpy as np
import cv2
from mtcnn.mtcnn import MTCNN
from sklearn.metrics.pairwise import cosine_similarity
import db
import config
from tensorflow.keras.applications.efficientnet import (
    preprocess_input as efficientnet_preprocess,
)
import logging

# Configure logging
logging.basicConfig(
    filename="face_recognition.log",
    level=logging.ERROR,
    format="%(asctime)s - %(levelname)s - %(message)s",
)

# --- C·∫§U H√åNH ---
# Import emotion labels from config
EMOTION_LABELS = config.EMOTION_LABELS
EMOTION_ICONS = config.EMOTION_ICONS

# --- Module-level cache for models ---
_CACHED_MODELS = None


# --- T·∫£i model ---
@st.cache_resource
def load_models():
    global _CACHED_MODELS
    if _CACHED_MODELS is not None:
        return _CACHED_MODELS

    print("‚ö° ƒêang t·∫£i models t·ªëi ∆∞u...")
    detector = MTCNN(min_face_size=60)
    try:
        full_model = tf.keras.models.load_model(
            config.MODEL_PATH,
            custom_objects={
                "preprocess_input": tf.keras.applications.efficientnet.preprocess_input
            },
            compile=False,
        )
        embed_model = tf.keras.Model(
            inputs=full_model.input,
            outputs=full_model.get_layer(config.EMBEDDING_LAYER_NAME).output,
        )
        print("‚úÖ ƒê√£ t·∫£i Face Recognition Model (B4)")
    except Exception as e:
        logging.exception("Model loading failed")
        st.error(f"L·ªói t·∫£i model Face: {e}")
        return None, None, None, None

    emotion_model, spoof_model = None, None
    if config.EMOTION_MODEL_PATH:
        try:
            emotion_model = tf.keras.models.load_model(
                config.EMOTION_MODEL_PATH,
                custom_objects={
                    "preprocess_input": tf.keras.applications.resnet.preprocess_input
                },
                compile=False,
            )
            print("‚úÖ ƒê√£ t·∫£i Emotion Model (ResNet50)")
        except Exception as e:
            logging.exception("Emotion model loading failed")
            print(f"‚ö†Ô∏è L·ªói Emotion: {e}")

    if config.SPOOF_MODEL_PATH:
        try:
            spoof_model = tf.keras.models.load_model(
                config.SPOOF_MODEL_PATH,
                custom_objects={
                    "preprocess_input": tf.keras.applications.resnet.preprocess_input
                },
                compile=False,
            )
            print("‚úÖ ƒê√£ t·∫£i Anti-Spoof Model (ResNet50)")
        except Exception as e:
            logging.exception("Spoof detection model loading failed")
            print(f"‚ö†Ô∏è L·ªói Spoof: {e}")

    _CACHED_MODELS = (detector, embed_model, spoof_model, emotion_model)
    return _CACHED_MODELS


# --- Helper Functions ---
def get_embedding(face_img_rgb):
    """T·∫°o vector ƒë·∫∑c tr∆∞ng t·ª´ ·∫£nh khu√¥n m·∫∑t.

    Args:
        face_img_rgb: ·∫¢nh khu√¥n m·∫∑t ƒê√É RESIZE v·ªÅ (224, 224) t·ª´ detect_and_align()

    Returns:
        Normalized embedding vector
    """
    _, embed_model, _, _ = load_models()

    if embed_model is None:
        logging.error("Embedding model is None")
        return None

    try:
        # ·∫¢nh ƒë√£ ƒë∆∞·ª£c resize t·ª´ detect_and_align() r·ªìi, kh√¥ng c·∫ßn resize l·∫°i
        face_tensor = np.expand_dims(face_img_rgb.astype("float32"), axis=0)
        face_tensor = tf.keras.applications.efficientnet.preprocess_input(face_tensor)

        # Predict
        embedding = embed_model(face_tensor, training=False)
        embedding = embedding.numpy()[0]

        return embedding / np.linalg.norm(embedding)
    except Exception as e:
        logging.exception(f"Error in get_embedding: {e}")
        return None


def recognize_from_crop(face_img_rgb, known_emb_matrix, known_names):
    """H√†m nh·∫≠n di·ªán nhanh d√πng cho v√≤ng l·∫∑p Real-time c·ªßa OpenCV.

    Args:
        face_img_rgb: ·∫¢nh m·∫∑t c·∫Øt (CH∆ØA resize) t·ª´ camera
        known_emb_matrix: Ma tr·∫≠n embeddings ƒë√£ bi·∫øt
        known_names: Danh s√°ch t√™n t∆∞∆°ng ·ª©ng

    Returns:
        (name, similarity_score)
    """
    if face_img_rgb.shape[0] < 20 or face_img_rgb.shape[1] < 20:
        return "Unknown", 0.0

    # Resize v·ªÅ 224x224 tr∆∞·ªõc khi t·∫°o embedding
    try:
        face_resized = cv2.resize(face_img_rgb, config.IMG_SIZE)
    except:
        return "Unknown", 0.0

    curr_emb = get_embedding(face_resized)
    if curr_emb is None:
        return "Unknown", 0.0

    # T√≠nh to√°n so kh·ªõp
    sims = cosine_similarity(curr_emb.reshape(1, -1), known_emb_matrix)[0]
    idx_max = np.argmax(sims)
    max_sim = sims[idx_max]

    if max_sim > config.COSINE_THRESHOLD:
        return known_names[idx_max], max_sim
    return "Unknown", max_sim


def detect_emotion(face_img_rgb):
    _, _, _, emotion_model = load_models()
    if emotion_model is None:
        return "N/A"
    try:
        face_resized = cv2.resize(face_img_rgb, config.EMOTION_IMG_SIZE)
        input_tensor = np.expand_dims(face_resized, axis=0).astype("float32")
        # Use ResNet preprocess for ResNet50 emotion model
        input_tensor = tf.keras.applications.resnet.preprocess_input(input_tensor)
        predictions = emotion_model(input_tensor, training=False).numpy()[0]
        if len(predictions) > 10:
            return "N/A"
        idx = np.argmax(predictions)
        return (
            f"{EMOTION_LABELS[idx]} {EMOTION_ICONS.get(EMOTION_LABELS[idx], '')}"
            if idx < len(EMOTION_LABELS)
            else "Unknown"
        )
    except:
        return "N/A"


# --- Pipeline Ch√≠nh ---
def detect_and_align(image_bytes=None, image_cv2=None):
    """
    H√†m ph√°t hi·ªán khu√¥n m·∫∑t, h·ªó tr·ª£ c·∫£ input l√† Bytes (Webcam Streamlit)
    v√† Numpy Array (OpenCV Real-time).
    """
    detector, _, _, _ = load_models()
    if detector is None:
        return None, None, None

    # ∆Øu ti√™n x·ª≠ l√Ω CV2 (Numpy) tr∆∞·ªõc n·∫øu c√≥
    if image_cv2 is not None:
        img_rgb = cv2.cvtColor(image_cv2, cv2.COLOR_BGR2RGB)
    elif image_bytes:
        image_bytes.seek(0)
        img = cv2.imdecode(
            np.frombuffer(image_bytes.read(), np.uint8), cv2.IMREAD_COLOR
        )
        if img is None:
            return None, None, None
        img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
    else:
        return None, None, None

    h_img, w_img = img_rgb.shape[:2]
    target_w = 640
    scale = target_w / float(w_img) if w_img > target_w else 1.0
    img_small = (
        cv2.resize(img_rgb, (0, 0), fx=scale, fy=scale) if scale < 1.0 else img_rgb
    )

    detections = detector.detect_faces(img_small)
    if not detections:
        return None, None, None

    # L·∫•y m·∫∑t to nh·∫•t
    detection = max(detections, key=lambda d: d["box"][2] * d["box"][3])
    x, y, w, h = detection["box"]

    # Scale t·ªça ƒë·ªô v·ªÅ ·∫£nh g·ªëc
    if scale < 1.0:
        x, y, w, h = int(x / scale), int(y / scale), int(w / scale), int(h / scale)

    # [QUAN TR·ªåNG] Th√™m Margin (L·ªÅ) 20% ƒë·ªÉ l·∫•y tr·ªçn khu√¥n m·∫∑t
    margin = 0.2
    x_new = max(0, int(x - w * margin))
    y_new = max(0, int(y - h * margin))
    w_new = min(w_img - x_new, int(w * (1 + 2 * margin)))
    h_new = min(h_img - y_new, int(h * (1 + 2 * margin)))

    face_img = img_rgb[y_new : y_new + h_new, x_new : x_new + w_new]

    try:
        face_resized = cv2.resize(face_img, config.IMG_SIZE)
    except:
        return None, None, None

    # Tr·∫£ v·ªÅ: ·∫¢nh m·∫∑t ƒë√£ resize (224x224), ·∫¢nh g·ªëc, T·ªça ƒë·ªô m·∫∑t g·ªëc (ƒë·ªÉ v·∫Ω khung)
    return face_resized, img_rgb, (x, y, w, h)


# --- UI Functions ---
def register_face(name, mssv, class_name, image_bytes):
    """ƒêƒÉng k√Ω khu√¥n m·∫∑t m·ªõi."""
    if not name or not mssv:
        return "Vui l√≤ng nh·∫≠p t√™n v√† MSSV."

    # G·ªçi h√†m detect
    face_img, _, _ = detect_and_align(image_bytes)

    if face_img is None:
        return "Kh√¥ng t√¨m th·∫•y khu√¥n m·∫∑t."
    try:
        # H√†m n√†y gi·ªù ƒë√£ an to√†n, t·ª± resize v·ªÅ 224x224
        embedding = get_embedding(face_img)
        if embedding is None:
            return "L·ªói x·ª≠ l√Ω ·∫£nh (Embedding)."

        db.save_user_data(name, mssv, class_name, embedding)
        return f"ƒêƒÉng k√Ω th√†nh c√¥ng: {name}"
    except Exception as e:
        return f"L·ªói: {e}"


def verify_face(
    image_bytes=None,
    input_class_name="",
    action_type="Check-in",
    enable_logging=True,
    image_cv2=None,
):
    """
    H√†m nh·∫≠n di·ªán v√† ƒëi·ªÉm danh.
    H·ªó tr·ª£ nh·∫≠n input t·ª´ c·∫£ Streamlit (bytes) v√† OpenCV (numpy array).
    """

    detector, _, spoof_model, _ = load_models()
    if detector is None:
        return "L·ªói Model", None, "N/A", 0.0, "N/A", False

    # 1. Detect & Align (T·ª± ƒë·ªông ch·ªçn ngu·ªìn ·∫£nh ph√π h·ª£p)
    face_img, img_rgb_full, coords = detect_and_align(
        image_bytes=image_bytes, image_cv2=image_cv2
    )

    # 2. Chu·∫©n b·ªã ·∫£nh v·∫Ω k·∫øt qu·∫£ (img_draw)
    img_draw = None
    if img_rgb_full is not None:
        img_draw = img_rgb_full.copy()  # ƒê√£ l√† RGB
    else:
        # Fallback: C·ªë g·∫Øng load ·∫£nh g·ªëc ƒë·ªÉ tr·∫£ v·ªÅ cho UI d√π kh√¥ng detect ƒë∆∞·ª£c m·∫∑t
        if image_cv2 is not None:
            img_draw = cv2.cvtColor(image_cv2, cv2.COLOR_BGR2RGB)
        elif image_bytes:
            image_bytes.seek(0)
            img_bgr = cv2.imdecode(
                np.frombuffer(image_bytes.read(), np.uint8), cv2.IMREAD_COLOR
            )
            if img_bgr is not None:
                img_draw = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2RGB)

    # Ki·ªÉm tra k·∫øt qu·∫£ detect
    if face_img is None or coords is None:
        return "Kh√¥ng t√¨m th·∫•y", img_draw, "N/A", 0.0, "N/A", False

    x, y, w, h = coords

    # L·ªçc ·∫£nh qu√° nh·ªè
    if face_img.shape[0] < 40 or face_img.shape[1] < 40:
        return "Kh√¥ng t√¨m th·∫•y", img_draw, "N/A", 0.0, "N/A", False

    # Ensure img_draw is valid before cv2 operations
    if img_draw is None:
        return "L·ªói ·∫£nh", None, "N/A", 0.0, "N/A", False

    # --- SPOOF CHECK ---
    is_real_face = True
    spoof_score = 0.0
    if spoof_model:
        try:
            spoof_input = cv2.resize(face_img, config.SPOOF_IMG_SIZE).astype("float32")
            spoof_input = np.expand_dims(spoof_input, axis=0)
            # Use ResNet preprocess for ResNet50 anti-spoof model
            spoof_input = tf.keras.applications.resnet.preprocess_input(spoof_input)
            spoof_pred = spoof_model(spoof_input, training=False).numpy()

            # Get real face score (binary classification: [fake, real])
            # Score > threshold = REAL, Score < threshold = FAKE
            spoof_score = (
                spoof_pred[0][1] if spoof_pred.shape[-1] == 2 else spoof_pred[0][0]
            )
            is_real_face = spoof_score > config.SPOOF_THRESHOLD

            # Log spoof result but DON'T early return - continue to recognition
            if is_real_face:
                print(
                    f"‚úÖ [SPOOF] Real face: score={spoof_score:.4f} (threshold={config.SPOOF_THRESHOLD})"
                )
            else:
                print(
                    f"‚ö†Ô∏è [SPOOF] Detected FAKE face: score={spoof_score:.4f} (threshold={config.SPOOF_THRESHOLD})"
                )
        except Exception as e:
            print(f"‚ö†Ô∏è [SPOOF] Error: {e}")
            spoof_score = 0.0
            is_real_face = False
            pass

    # --- RECOGNITION ---
    live_emb = get_embedding(face_img)
    if live_emb is None:
        return "L·ªói ·∫£nh", img_draw, "N/A", 0.0, "N/A", False

    known_embeddings = db.load_embeddings()
    best_name = "Unknown"
    max_sim = 0.0

    if known_embeddings:
        names = list(known_embeddings.keys())
        emb_matrix = np.array(list(known_embeddings.values()))
        sims = cosine_similarity(live_emb.reshape(1, -1), emb_matrix)[0]
        idx_max = np.argmax(sims)
        max_sim = sims[idx_max]
        if max_sim > config.COSINE_THRESHOLD:
            best_name = names[idx_max]

    print(f"üë§ [REC] Name: {best_name} | Sim: {max_sim:.4f}")
    emotion = detect_emotion(face_img)

    # --- LOGIC GHI LOG ---
    # Ki·ªÉm tra c·∫£ cosine similarity V√Ä anti-spoof score
    pass_cosine = max_sim > config.COSINE_THRESHOLD
    pass_spoof = is_real_face  # Already checked against SPOOF_THRESHOLD
    both_pass = pass_cosine and pass_spoof

    color = (0, 255, 0) if (best_name != "Unknown" and both_pass) else (255, 0, 0)
    action_log, has_new_checkin = "N/A", False

    if best_name != "Unknown":
        if not both_pass:
            # Nh·∫≠n di·ªán ƒë∆∞·ª£c nh∆∞ng kh√¥ng ƒë·∫°t threshold - hi·ªÉn th·ªã C·∫¢ HAI ƒëi·ªÅu ki·ªán
            fail_reasons = []
            if not pass_cosine:
                fail_reasons.append(f"Cosine {max_sim:.3f} < {config.COSINE_THRESHOLD}")
                print(f"‚ö†Ô∏è [CHECK] {best_name}: Cosine failed ({max_sim:.3f})")
            if not pass_spoof:
                fail_reasons.append(
                    f"Spoof {spoof_score:.3f} < {config.SPOOF_THRESHOLD}"
                )
                print(f"‚ö†Ô∏è [CHECK] {best_name}: Spoof check failed ({spoof_score:.3f})")

            action_log = f"‚ö†Ô∏è Kh√¥ng ƒë·∫°t: {' & '.join(fail_reasons)}"
        else:
            # C·∫¢ HAI ƒê·ªÄU PASS - Cho ph√©p ghi log
            mssv, r_class = db.get_user_info(best_name)
            final_class = input_class_name if input_class_name else r_class
            last_action = db.get_last_action(best_name)

            if action_type == "Check-in":
                if last_action != "Check-in":
                    action_log = f"S·∫µn s√†ng Check-in: {best_name}"
                else:
                    action_log = f"‚ö†Ô∏è {best_name} ƒë√£ Check-in r·ªìi."
            else:  # Check-out
                if last_action == "Check-in":
                    action_log = f"S·∫µn s√†ng Check-out: {best_name}"
                else:
                    action_log = f"‚ö†Ô∏è {best_name} ch∆∞a th·ªÉ Check-out."

            # Ch·ªâ ghi log n·∫øu enable_logging = True V√Ä c·∫£ 2 threshold ƒë·ªÅu pass
            if enable_logging and both_pass:
                if action_type == "Check-in" and last_action != "Check-in":
                    db.log_attendance(
                        best_name,
                        mssv,
                        final_class,
                        "Check-in",
                        max_sim,
                        spoof_score,
                        emotion,
                    )
                    has_new_checkin = True
                    action_log = "‚úÖ Check-in th√†nh c√¥ng!"
                    print(
                        f"‚úÖ [LOG] Check-in: {best_name} (cos={max_sim:.3f}, spoof={spoof_score:.3f})"
                    )
                elif action_type == "Check-out" and last_action == "Check-in":
                    db.log_attendance(
                        best_name,
                        mssv,
                        final_class,
                        "Check-out",
                        max_sim,
                        spoof_score,
                        emotion,
                    )
                    has_new_checkin = True
                    action_log = "‚úÖ Check-out th√†nh c√¥ng!"
                    print(
                        f"‚úÖ [LOG] Check-out: {best_name} (cos={max_sim:.3f}, spoof={spoof_score:.3f})"
                    )

    # V·∫Ω Box v·ªõi m√†u t√πy theo k·∫øt qu·∫£
    cv2.rectangle(img_draw, (x, y), (x + w, y + h), color, 2)

    # Label ch√≠nh: T√™n + Cosine score
    label = f"{best_name} (cos:{max_sim:.2f})" if best_name != "Unknown" else "NGUOI LA"
    cv2.putText(img_draw, label, (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.7, color, 2)

    # Label ph·ª•: Anti-spoof score (hi·ªÉn th·ªã lu√¥n n·∫øu c√≥ spoof model)
    if spoof_model and spoof_score > 0:
        spoof_label = f"Spoof: {spoof_score:.3f}"
        spoof_color = (0, 255, 0) if is_real_face else (255, 0, 0)
        cv2.putText(
            img_draw,
            spoof_label,
            (x, y - 35),
            cv2.FONT_HERSHEY_SIMPLEX,
            0.6,
            spoof_color,
            2,
        )
    if "Wrong" not in emotion and "N/A" not in emotion:
        cv2.putText(
            img_draw,
            emotion.split(" ")[0],
            (x, y + h + 25),
            cv2.FONT_HERSHEY_SIMPLEX,
            0.7,
            (255, 255, 0),
            2,
        )

    return best_name, img_draw, emotion, max_sim, action_log, has_new_checkin
