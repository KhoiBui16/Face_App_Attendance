import streamlit as st
import tensorflow as tf
import numpy as np
import cv2
from mtcnn.mtcnn import MTCNN
from sklearn.metrics.pairwise import cosine_similarity
import db

# --- H·∫±ng s·ªë ---
MODEL_PATH = "models/ResNet50_feature_extractor.keras"
EMBEDDING_LAYER_NAME = "cnn_embedding"
IMG_SIZE = (224, 224)
COSINE_THRESHOLD = 0.8

# --- H·∫±ng s·ªë cho 2 model m·ªõi ---
# (!!!) CH√ö √ù: ƒê·∫∑t t√™n file model c·ªßa b·∫°n v√†o ƒë√¢y
SPOOF_MODEL_PATH = "models/anti_spoof_model.h5"
EMOTION_MODEL_PATH = "models/emotion_model.h5"

# (!!!) CH√ö √ù: C√°c th√¥ng s·ªë n√†y PH·∫¢I KH·ªöP v·ªõi model b·∫°n t·∫£i v·ªÅ
SPOOF_IMG_SIZE = (224, 224)  # Gi·∫£ s·ª≠ model spoof d√πng 224x224
EMOTION_IMG_SIZE = (48, 48)  # Model emotion (FER2013) th∆∞·ªùng l√† 48x48
EMOTION_LABELS = ["Angry", "Disgust", "Fear", "Happy", "Sad", "Surprise", "Neutral"]
EMOTION_ICONS = {
    "Happy": "üòä",
    "Sad": "üò¢",
    "Angry": "üò†",
    "Surprise": "üòÆ",
    "Neutral": "üòê",
    "Fear": "üò®",
    "Disgust": "ü§¢",
}


# --- T·∫£i model (C·∫≠p nh·∫≠t) ---
@st.cache_resource
def load_models():
    print("ƒêang t·∫£i models...")
    detector = MTCNN()

    # 1. Model Embedding (C·ªßa b·∫°n)
    try:
        full_model = tf.keras.models.load_model(MODEL_PATH)
        embed_model = tf.keras.Model(
            inputs=full_model.input,
            outputs=full_model.get_layer(EMBEDDING_LAYER_NAME).output,
        )
    except Exception as e:
        st.error(f"L·ªói khi t·∫£i model embedding: {e}.")
        return None, None, None, None

    # 2. Model Anti-Spoof
    try:
        spoof_model = tf.keras.models.load_model(SPOOF_MODEL_PATH)
        print(f"T·∫£i model Anti-Spoof '{SPOOF_MODEL_PATH}' th√†nh c√¥ng.")
    except Exception as e:
        print(f"Kh√¥ng t√¨m th·∫•y model anti-spoof t·∫°i '{SPOOF_MODEL_PATH}'. B·ªè qua...")
        spoof_model = None

    # 3. Model Emotion Detection
    try:
        emotion_model = tf.keras.models.load_model(EMOTION_MODEL_PATH)
        print(f"T·∫£i model Emotion '{EMOTION_MODEL_PATH}' th√†nh c√¥ng.")
    except Exception as e:
        print(f"Kh√¥ng t√¨m th·∫•y model emotion t·∫°i '{EMOTION_MODEL_PATH}'. B·ªè qua...")
        emotion_model = None

    print("T·∫£i models th√†nh c√¥ng.")
    return detector, embed_model, spoof_model, emotion_model


# --- C√°c h√†m Pipeline ---
def detect_and_align(image_bytes):
    """
    Ph√°t hi·ªán khu√¥n m·∫∑t, tr·∫£ v·ªÅ ·∫£nh ƒë√£ c·∫Øt V√Ä t·ªça ƒë·ªô.
    """
    detector, _, _, _ = load_models()
    if detector is None:
        return None, None, None

    # --- (ƒê√É TH√äM) ---
    # Tua l·∫°i file stream v·ªÅ ƒë·∫ßu tr∆∞·ªõc khi ƒë·ªçc
    image_bytes.seek(0)

    img = cv2.imdecode(np.frombuffer(image_bytes.read(), np.uint8), cv2.IMREAD_COLOR)
    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
    detections = detector.detect_faces(img_rgb)

    if not detections:
        return None, None, None  # Kh√¥ng t√¨m th·∫•y m·∫∑t

    detection = detections[0]
    x, y, w, h = detection["box"]
    face_coords = (x, y, w, h)  # T·ªça ƒë·ªô khu√¥n m·∫∑t

    face_img = img_rgb[y : y + h, x : x + w]
    face_img_resized = cv2.resize(face_img, IMG_SIZE)

    return face_img_resized, img_rgb, face_coords


def check_anti_spoof(face_img_rgb):
    _, _, spoof_model, _ = load_models()
    if spoof_model is None:
        print("B·ªè qua anti-spoofing (ch∆∞a c√≥ model).")
        return False
    face_resized_spoof = cv2.resize(face_img_rgb, SPOOF_IMG_SIZE)
    input_tensor = np.expand_dims(face_resized_spoof, axis=0)
    input_tensor = input_tensor / 255.0
    prediction = spoof_model.predict(input_tensor)[0][0]
    SPOOF_THRESHOLD = 0.8
    if prediction > SPOOF_THRESHOLD:
        print(f"Ph√°t hi·ªán Spoof! Score: {prediction:.2f}")
        return True
    else:
        print(f"·∫¢nh th·∫≠t. Score: {prediction:.2f}")
        return False


def get_embedding(face_img_rgb):
    _, embed_model, _, _ = load_models()
    face_tensor = np.expand_dims(face_img_rgb, axis=0)
    face_tensor_preprocessed = tf.keras.applications.resnet.preprocess_input(
        face_tensor
    )
    embedding = embed_model.predict(face_tensor_preprocessed)[0]
    embedding = embedding / np.linalg.norm(embedding)
    return embedding


def detect_emotion(face_img_rgb):
    """(C·∫≠p nh·∫≠t) Ph√°t hi·ªán c·∫£m x√∫c."""
    _, _, _, emotion_model = load_models()
    if emotion_model is None:
        return "N/A"

    # --- Ti·ªÅn x·ª≠ l√Ω cho model Emotion ---
    # 1. Chuy·ªÉn v·ªÅ ·∫£nh x√°m (Grayscale)
    face_gray = cv2.cvtColor(face_img_rgb, cv2.COLOR_RGB2GRAY)

    # 2. Resize v·ªÅ 48x48
    face_resized_emotion = cv2.resize(face_gray, EMOTION_IMG_SIZE)

    # 3. Chu·∫©n h√≥a [0, 1]
    input_tensor = face_resized_emotion / 255.0

    # 4. Th√™m chi·ªÅu batch (1) v√† chi·ªÅu k√™nh (1)
    input_tensor = np.expand_dims(input_tensor, axis=-1)  # (48, 48) -> (48, 48, 1)
    input_tensor = np.expand_dims(input_tensor, axis=0)  # (48, 48, 1) -> (1, 48, 48, 1)

    # 5. D·ª± ƒëo√°n
    predictions = emotion_model.predict(input_tensor)[0]
    emotion_index = np.argmax(predictions)
    emotion_text = EMOTION_LABELS[emotion_index]

    print(f"Ph√°t hi·ªán c·∫£m x√∫c: {emotion_text} ({predictions[emotion_index]:.2f})")
    return f"{emotion_text} {EMOTION_ICONS.get(emotion_text, '')}"


# --- C√°c h√†m ch√≠nh cho UI (C·∫≠p nh·∫≠t) ---
def register_face(name, image_bytes):
    if not name:
        return "Vui l√≤ng nh·∫≠p t√™n."

    # face_img_224x224 l√† ·∫£nh ƒë√£ resize (224, 224, 3)
    # img_rgb_original l√† ·∫£nh g·ªëc (t·ª´ webcam)
    face_img_224x224, _, img_rgb_original = detect_and_align(image_bytes)

    if face_img_224x224 is None:
        return "Kh√¥ng ph√°t hi·ªán th·∫•y khu√¥n m·∫∑t."

    # Ch·∫°y anti-spoof tr√™n ·∫£nh g·ªëc (ch·∫•t l∆∞·ª£ng cao h∆°n)
    if check_anti_spoof(img_rgb_original):
        return "Ph√°t hi·ªán gi·∫£ m·∫°o (spoof)! ƒêƒÉng k√Ω th·∫•t b·∫°i."

    embedding = get_embedding(face_img_224x224)
    try:
        db.save_embedding(name, embedding)
        return f"ƒêƒÉng k√Ω th√†nh c√¥ng cho {name}!"
    except Exception as e:
        return f"L·ªói khi l∆∞u embedding: {e}"


def verify_face(image_bytes):
    """
    Th·ª±c hi·ªán pipeline, V√Ä V·∫º Bounding Box (ƒê·ªè/Xanh)
    """
    # 1. Detect v√† l·∫•y t·ªça ƒë·ªô
    face_img_224x224, img_to_draw_on, face_coords = detect_and_align(image_bytes)

    action_taken = "N/A"

    if face_img_224x224 is None:
        db.log_attendance("N/A", "Kh√¥ng t√¨m th·∫•y khu√¥n m·∫∑t", 0.0, "N/A")

        # --- (ƒê√É S·ª¨A L·ªñI CRASH) ---
        # Tua l·∫°i file stream v·ªÅ ƒë·∫ßu
        image_bytes.seek(0)

        # ƒê·ªçc l·∫°i ·∫£nh g·ªëc ƒë·ªÉ hi·ªÉn th·ªã
        img_rgb_original = cv2.imdecode(
            np.frombuffer(image_bytes.read(), np.uint8), cv2.IMREAD_COLOR
        )
        annotated_img = cv2.cvtColor(img_rgb_original, cv2.COLOR_BGR2RGB)
        return "Kh√¥ng t√¨m th·∫•y", annotated_img, "N/A", 0.0, action_taken

    # 2. Check Anti-spoof
    if check_anti_spoof(img_to_draw_on):
        db.log_attendance("N/A", "Gi·∫£ m·∫°o (Spoof)", 0.0, "N/A")
        action_taken = "Gi·∫£ m·∫°o (Spoof)"

        x, y, w, h = face_coords
        cv2.rectangle(img_to_draw_on, (x, y), (x + w, y + h), (255, 0, 0), 2)
        cv2.putText(
            img_to_draw_on,
            "SPOOF DETECTED",
            (x, y - 10),
            cv2.FONT_HERSHEY_SIMPLEX,
            0.9,
            (255, 0, 0),
            2,
        )
        return "Gi·∫£ m·∫°o (Spoof)", img_to_draw_on, "N/A", 0.0, action_taken

    # 3. L·∫•y embedding v√† so s√°nh
    live_embedding = get_embedding(face_img_224x224)
    known_embeddings = db.load_embeddings()

    if not known_embeddings:
        x, y, w, h = face_coords
        cv2.rectangle(img_to_draw_on, (x, y), (x + w, y + h), (255, 0, 0), 2)
        cv2.putText(
            img_to_draw_on,
            "NGUOI LA (Stranger)",
            (x, y - 10),
            cv2.FONT_HERSHEY_SIMPLEX,
            0.9,
            (255, 0, 0),
            2,
        )
        return "Kh√¥ng c√≥ CSDL", img_to_draw_on, "N/A", 0.0, action_taken

    # ... (Ph·∫ßn c√≤n l·∫°i c·ªßa h√†m verify_face gi·ªØ nguy√™n) ...
    best_match_name = "Kh√¥ng nh·∫≠n di·ªán ƒë∆∞·ª£c"
    max_sim = 0.0
    for name, saved_embedding in known_embeddings.items():
        sim = cosine_similarity(
            live_embedding.reshape(1, -1), saved_embedding.reshape(1, -1)
        )[0][0]
        if sim > max_sim:
            max_sim = sim
            if sim > COSINE_THRESHOLD:
                best_match_name = name

    emotion = "N/A"
    display_status = ""
    color = (255, 0, 0)  # M·∫∑c ƒë·ªãnh l√† ƒë·ªè

    if best_match_name != "Kh√¥ng nh·∫≠n di·ªán ƒë∆∞·ª£c":
        display_status = best_match_name
        color = (0, 255, 0)  # Xanh
        emotion = detect_emotion(face_img_224x224)
        last_action = db.get_last_action(best_match_name)
        if last_action is None or last_action == "Check-out":
            action_taken = "Check-in"
        elif last_action == "Check-in":
            action_taken = "Check-out"
        db.log_attendance(best_match_name, action_taken, max_sim, emotion)
    else:
        display_status = "NG∆Ø·ªúI L·∫† (Stranger)"
        color = (255, 0, 0)  # ƒê·ªè
        action_taken = "Nh·∫≠n di·ªán th·∫•t b·∫°i"
        db.log_attendance("Ng∆∞·ªùi l·∫°", action_taken, max_sim, emotion)

    x, y, w, h = face_coords
    cv2.rectangle(img_to_draw_on, (x, y), (x + w, y + h), color, 2)
    cv2.putText(
        img_to_draw_on,
        display_status,
        (x, y - 10),
        cv2.FONT_HERSHEY_SIMPLEX,
        0.9,
        color,
        2,
    )
    return display_status, img_to_draw_on, emotion, max_sim, action_taken
